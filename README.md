## Web Crawler Module

The system includes a web crawler for dynamic knowledge base updates.

### Usage Guidelines
- **Educational use only** - designed for learning AI/RAG concepts
- **Respects rate limits** - built-in delays prevent server overload
- **Wikipedia focused** - uses educational sources by default
- **Check robots.txt** - always verify crawling permissions

### Legal Compliance
Users must:
- Respect website terms of service
- Check robots.txt before crawling
- Implement appropriate rate limiting
- Consider copyright implications

### Crawler Features
- Automatic content extraction and cleaning
- Scheduled crawling capabilities
- Integration with ChromaDB vector storage
- Chunking and metadata management